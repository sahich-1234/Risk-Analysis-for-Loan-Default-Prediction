{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471064cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "app_train = pd.read_csv(\"C:/Users/SMRUTI DESHPANDE/house credit default/application_train.csv\")\n",
    "app_test = pd.read_csv(\"C:/Users/SMRUTI DESHPANDE/house credit default/application_test.csv\")\n",
    "credit_card_balance = pd.read_csv(\"C:/Users/SMRUTI DESHPANDE/house credit default/credit_card_balance.csv\")\n",
    "bureau = pd.read_csv(\"C:/Users/SMRUTI DESHPANDE/house credit default/bureau.csv\")\n",
    "previous = pd.read_csv(\"C:/Users/SMRUTI DESHPANDE/house credit default/previous_application.csv\")\n",
    "installments = pd.read_csv(\"C:/Users/SMRUTI DESHPANDE/house credit default/installments_payments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820ab2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: dask[dataframe] in c:\\programdata\\anaconda3\\lib\\site-packages (2022.7.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (2022.11.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (1.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (22.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (0.12.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (2.0.0)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\smruti deshpande\\appdata\\roaming\\python\\python310\\site-packages (from dask[dataframe]) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0->dask[dataframe]) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0->dask[dataframe]) (2.8.2)\n",
      "Requirement already satisfied: locket in c:\\programdata\\anaconda3\\lib\\site-packages (from partd>=0.3.10->dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0->dask[dataframe]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install dask[dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3413cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_BALANCE', 'AMT_ANNUITY', 'SK_DPD', 'CNT_CHILDREN', 'FLAG_OWN_CAR', 'CODE_GENDER', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE', 'NAME_HOUSING_TYPE', 'NAME_CONTRACT_TYPE']\n",
      "Pipeline saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to reduce memory usage\n",
    "def reduce_memory_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:  # Exclude string columns\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            elif pd.api.types.is_float_dtype(col_type):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df\n",
    "\n",
    "# Reduce memory usage\n",
    "app_train = reduce_memory_usage(app_train)\n",
    "app_test = reduce_memory_usage(app_test)\n",
    "credit_card_balance = reduce_memory_usage(credit_card_balance)\n",
    "\n",
    "# Merge data\n",
    "columns_to_merge = ['SK_ID_CURR', 'AMT_BALANCE', 'SK_DPD']\n",
    "credit_card_balance_selected = credit_card_balance[columns_to_merge]\n",
    "\n",
    "app_train_dd = dd.from_pandas(app_train, npartitions=10)\n",
    "credit_card_balance_dd = dd.from_pandas(credit_card_balance_selected, npartitions=10)\n",
    "app_test_dd = dd.from_pandas(app_test[['SK_ID_CURR']], npartitions=10)\n",
    "\n",
    "merged_data_dd = dd.merge(app_train_dd, credit_card_balance_dd, on='SK_ID_CURR', how='left')\n",
    "merged_data_dd = dd.merge(merged_data_dd, app_test_dd, on='SK_ID_CURR', how='left')\n",
    "merged_data = merged_data_dd.compute()\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "merged_data['AMT_BALANCE'] = imputer.fit_transform(merged_data[['AMT_BALANCE']])\n",
    "merged_data['SK_DPD'] = SimpleImputer(strategy='median').fit_transform(merged_data[['SK_DPD']])\n",
    "\n",
    "# Map binary values to numeric\n",
    "binary_map = {'Y': 1, 'N': 0, 'M': 0, 'F': 1}\n",
    "merged_data['FLAG_OWN_CAR'] = merged_data['FLAG_OWN_CAR'].map(binary_map)\n",
    "merged_data['CODE_GENDER'] = merged_data['CODE_GENDER'].map(binary_map)\n",
    "\n",
    "# Define feature columns\n",
    "input_parameters = [\n",
    "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_BALANCE', 'AMT_ANNUITY', 'SK_DPD', 'CNT_CHILDREN',\n",
    "    'FLAG_OWN_CAR', 'CODE_GENDER', 'DAYS_CREDIT', 'DAYS_DECISION', 'AMT_PAYMENT', \n",
    "    'AMT_INSTALMENT', 'AMT_APPLICATION', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE', \n",
    "    'NAME_HOUSING_TYPE', 'NAME_CONTRACT_TYPE'\n",
    "]\n",
    "\n",
    "# Filter columns available in merged_data\n",
    "available_columns = [col for col in input_parameters if col in merged_data.columns]\n",
    "\n",
    "# Print available columns for debugging\n",
    "print(\"Available columns:\", available_columns)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = [\n",
    "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_BALANCE', 'AMT_ANNUITY', 'SK_DPD', \n",
    "    'CNT_CHILDREN'\n",
    "]\n",
    "categorical_features = ['NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE', 'NAME_HOUSING_TYPE', 'NAME_CONTRACT_TYPE']\n",
    "\n",
    "# Split data into features and target\n",
    "training_data = merged_data[available_columns + ['TARGET']]\n",
    "X = training_data[available_columns]\n",
    "y = training_data['TARGET']\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a full pipeline that includes preprocessing and the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the pipeline\n",
    "save_dir = 'C:\\\\Users\\\\SMRUTI DESHPANDE\\\\house credit default\\\\'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "joblib.dump(pipeline, os.path.join(save_dir, 'credit_model_pipeline.pkl'))\n",
    "\n",
    "print(\"Pipeline saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bbaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
